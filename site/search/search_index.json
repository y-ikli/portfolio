{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Younes IKLI \u2014 Data &amp; Analytics Engineer","text":""},{"location":"#a-propos","title":"\u00c0 propos","text":"<p>Ing\u00e9nieur Data sp\u00e9cialis\u00e9 dans la Modern Data Stack, j'\u00e9volue \u00e0 la fronti\u00e8re entre l'ing\u00e9nierie d'infrastructure et la mod\u00e9lisation m\u00e9tier. Mon objectif : transformer des flux de donn\u00e9es brutes complexes en actifs strat\u00e9giques fiables et actionnables.</p>"},{"location":"#expertise-hybride","title":"Expertise Hybride","text":""},{"location":"#expertise-data","title":"Expertise Data","text":""},{"location":"#expertise-data_1","title":"Expertise Data","text":"\ud83c\udfd7\ufe0f Data Engineering \ud83d\udcca Analytics Engineering Objectif : Fiabilit\u00e9 des pipelines Objectif : Qualit\u00e9 des donn\u00e9es analytiques Ingestion de donn\u00e9es multi-sources Mod\u00e9lisation de donn\u00e9es orient\u00e9e analyse Pipelines ELT orchestr\u00e9s (Airflow) Transformations SQL avec dbt Automatisation et structuration des flux D\u00e9finition et documentation de datasets CI/CD avec GitLab CI Tests de donn\u00e9es et contr\u00f4les de coh\u00e9rence Outils : Python, Airflow, Docker, Cloud DWH Outils : SQL, dbt, BigQuery / Snowflake"},{"location":"#projet-phare-marketing-data-platform","title":"Projet Phare : Marketing Data Platform","text":"<p>Projet personnel visant \u00e0 reproduire les patterns d\u2019une data platform moderne en conditions proches de la production.</p> <p>Plateforme end-to-end unifiant les donn\u00e9es de r\u00e9gies h\u00e9t\u00e9rog\u00e8nes (Google Ads, Meta, etc.) dans un entrep\u00f4t de donn\u00e9es cloud, depuis l\u2019ingestion des donn\u00e9es brutes jusqu\u2019\u00e0 la production de datasets analytiques fiables.</p>"},{"location":"#pourquoi-ce-projet-est-unique","title":"Pourquoi ce projet est unique :","text":"<p>Ce projet illustre ma capacit\u00e9 \u00e0 g\u00e9rer un cycle de vie data complet avec deux angles de lecture :</p> <ul> <li>L'excellence op\u00e9rationnelle (DE) : Architecture modulaire permettant d'ajouter une source en moins d'un jour, ingestion idempotente et monitoring de volum\u00e9trie.</li> <li>La pr\u00e9cision analytique (AE) : Unification de sch\u00e9mas disparates dans un Data Mart unique, garantissant des KPIs (ROAS, CPA) 100% fiables gr\u00e2ce aux tests dbt.</li> </ul> <p>\u27a1\ufe0f Explorer le projet | Voir le code sur GitHub</p>"},{"location":"experiences/","title":"Contexte professionnel et technique","text":"<p>Cette page apporte un compl\u00e9ment de contexte au portfolio. Elle synth\u00e9tise mon parcours et mes comp\u00e9tences sous l\u2019angle de la conception de plateformes data, de la construction de pipelines fiables et de la production de donn\u00e9es analytiques exploitables, dans une logique Data Engineer / Analytics Engineer.</p>"},{"location":"experiences/#experiences-cles","title":"Exp\u00e9riences cl\u00e9s","text":""},{"location":"experiences/#inria-data-engineer-data-platform","title":"Inria \u2014 Data Engineer (Data Platform)","text":"<ul> <li>Conception et \u00e9volution d\u2019une data platform cloud orient\u00e9e production.</li> <li>D\u00e9veloppement de pipelines Python / SQL avec tests, documentation et contr\u00f4les de coh\u00e9rence.</li> <li>Structuration de datasets analytiques utilis\u00e9s par des \u00e9quipes internes.</li> <li>Collaboration avec des profils data et techniques pour garantir stabilit\u00e9 et exploitabilit\u00e9.</li> </ul>"},{"location":"experiences/#apteeus-data-engineer","title":"Apteeus \u2014 Data Engineer","text":"<ul> <li>D\u00e9veloppement de pipelines ETL pour donn\u00e9es structur\u00e9es (TXT, CSV, Excel).</li> <li>Transformation, mise en qualit\u00e9 et documentation de donn\u00e9es analytiques.</li> <li>Construction de bases coh\u00e9rentes et maintenables dans le temps.</li> </ul>"},{"location":"experiences/#alicante-analytics-engineer","title":"Alicante \u2014 Analytics Engineer","text":"<ul> <li>Ingestion et normalisation de sources tabulaires h\u00e9t\u00e9rog\u00e8nes.</li> <li>Conception de transformations analytiques automatis\u00e9es.</li> <li>Production de datasets exploitables par des utilisateurs m\u00e9tier.</li> </ul>"},{"location":"experiences/#projets-significatifs","title":"Projets significatifs","text":""},{"location":"experiences/#data-platform-cas-dusage-agence-media","title":"Data Platform \u2014 Cas d\u2019usage agence m\u00e9dia","text":"<ul> <li>Plateforme data simulant un contexte d\u2019agence m\u00e9dia.</li> <li>Centralisation de sources h\u00e9t\u00e9rog\u00e8nes et automatisation ingestion / transformation.</li> <li>Production de datasets analytiques structur\u00e9s et pr\u00eats \u00e0 l\u2019usage.</li> </ul>"},{"location":"experiences/#chu-de-lille-projet-data-master-2-data-science","title":"CHU de Lille \u2014 Projet Data (Master 2 Data Science)","text":"<ul> <li>Pr\u00e9paration et analyse de donn\u00e9es issues de signaux physiologiques.</li> <li>Mise en \u0153uvre de m\u00e9thodes de classification pour l\u2019anticipation de crises d\u2019\u00e9pilepsie.</li> <li>Attention port\u00e9e \u00e0 la qualit\u00e9, la coh\u00e9rence et la reproductibilit\u00e9.</li> <li>Collaboration avec des profils techniques et non techniques.</li> </ul>"},{"location":"experiences/#competences-techniques","title":"Comp\u00e9tences techniques","text":"<p>Data Engineering - Ingestion multi-sources et pipelines de donn\u00e9es - Orchestration et automatisation des traitements - Fiabilit\u00e9, gestion des erreurs et performance - Data platforms et bases de donn\u00e9es analytiques</p> <p>Analytics Engineering - Transformations analytiques SQL-first - Mod\u00e9lisation et structuration des donn\u00e9es - Tables analytiques, tests et contr\u00f4les de qualit\u00e9 - Documentation et exploitation des datasets</p> <p>Outils et environnements - Langages : Python, SQL - Orchestration : Airflow - Transformation analytique : dbt - Cloud &amp; data platforms : GCP (BigQuery), Snowflake - Bases de donn\u00e9es : PostgreSQL, MongoDB - Qualit\u00e9 &amp; tests : dbt tests, tests SQL, pytest - Conteneurisation &amp; CI/CD : Docker, GitHub Actions, GitLab CI - Collaboration : Git, GitHub, GitLab - Environnement : Linux, Bash</p>"},{"location":"experiences/#transmission-et-engagements","title":"Transmission et engagements","text":"<ul> <li>Tutorat universitaire en programmation Python.</li> <li>Vulgarisation de concepts data et accompagnement de bonnes pratiques.</li> <li>Engagement associatif autour du logiciel libre et de projets collaboratifs.</li> </ul>"},{"location":"experiences/#synthese","title":"Synth\u00e8se","text":"<ul> <li>Conception et exploitation de plateformes et pipelines data.</li> <li>Structuration et transformation de donn\u00e9es \u00e0 des fins analytiques.</li> <li>Forte attention \u00e0 la qualit\u00e9, la fiabilit\u00e9 et la documentation.</li> <li>Pratique r\u00e9guli\u00e8re de la collaboration et de la transmission technique.</li> </ul>"},{"location":"projets/","title":"Focus Projet : Marketing Data Platform","text":"<p>L'objectif : Construire une infrastructure robuste capable d'unifier les donn\u00e9es de Google Ads et Meta Ads pour fournir une vision business transverse, fiable et automatis\u00e9e.</p> <p>Stack : Python, Airflow, dbt, BigQuery, Docker, GitHub Actions.</p>"},{"location":"projets/#architecture-technique","title":"Architecture Technique","text":"<p>Cette plateforme repose sur un d\u00e9couplage strict entre le transport de la donn\u00e9e et sa valorisation :</p> <ol> <li>Ingestion (Python + Airflow) : Extraction via des connecteurs modulaires, gestion de l'idempotence et chargement dans le <code>RAW Layer</code> de BigQuery.</li> <li>Transformation (dbt) : Passage par 4 couches de mod\u00e9lisation (Staging, Intermediate, Marts) pour garantir la qualit\u00e9 et la r\u00e9utilisabilit\u00e9.</li> <li>Orchestration : Un DAG Airflow centralise le cycle de vie, avec des Task Groups pour parall\u00e9liser l'ingestion.</li> </ol>"},{"location":"projets/#apercu-de-la-plateforme","title":"Aper\u00e7u de la plateforme","text":""},{"location":"projets/#1-orchestration-monitoring-airflow","title":"1. Orchestration &amp; Monitoring (Airflow)","text":"<p> Vue du DAG orchestrant l'ingestion parall\u00e9lis\u00e9e et le d\u00e9clenchement des transformations dbt. Chaque \u00e9tape inclut une logique de retry et un logging d\u00e9taill\u00e9.</p>"},{"location":"projets/#2-modelisation-lignage-dbt","title":"2. Mod\u00e9lisation &amp; Lignage (dbt)","text":"<p> Structure des transformations en couches. Le passage du <code>Staging</code> au <code>Mart</code> permet d'isoler les r\u00e8gles de gestion m\u00e9tier de la structure brute des APIs.</p>"},{"location":"projets/#3-qualite-exposition-bigquery","title":"3. Qualit\u00e9 &amp; Exposition (BigQuery)","text":"<p> Exposition des Data Marts finaux dans BigQuery. Les donn\u00e9es sont nettoy\u00e9es, typ\u00e9es et pr\u00eates pour \u00eatre consomm\u00e9es par un outil de BI (Looker, Tableau).</p>"},{"location":"projets/#le-regard-data-engineer-infrastructure","title":"\ud83d\udee0\ufe0f Le Regard \"Data Engineer\" (Infrastructure)","text":"<p>Comment j'ai assur\u00e9 la robustesse du syst\u00e8me :</p> <ul> <li>Design Pattern \"Connector\" : D\u00e9veloppement d'une interface Python abstraite. L'ajout d'une nouvelle source se fait par configuration, garantissant une maintenance simplifi\u00e9e.</li> <li>Ingestion Idempotente : Utilisation de strat\u00e9gies de chargement <code>Write-Truncate</code> sur partitions quotidiennes pour permettre de relancer n'importe quel pipeline sans risque de doublons.</li> <li>Contr\u00f4les de Volum\u00e9trie : Script de monitoring comparant le nombre de lignes ing\u00e9r\u00e9es avec les moyennes historiques pour d\u00e9tecter les anomalies d'API.</li> </ul>"},{"location":"projets/#le-regard-analytics-engineer-modelisation","title":"\ud83d\udcca Le Regard \"Analytics Engineer\" (Mod\u00e9lisation)","text":"<p>Comment j'ai transform\u00e9 la donn\u00e9e en actif m\u00e9tier :</p> <ul> <li>Unification Cross-Canal : Harmonisation des sch\u00e9mas de Google et Meta (ex: <code>spend</code>, <code>clicks</code>, <code>impressions</code>) dans un mod\u00e8le unique pour calculer un ROAS global.</li> <li>Data Quality as a Code :<ul> <li>Impl\u00e9mentation de tests <code>not_null</code> et <code>unique</code> sur les cl\u00e9s primaires.</li> <li>Tests de coh\u00e9rence m\u00e9tier (ex: le co\u00fbt ne peut pas \u00eatre n\u00e9gatif).</li> </ul> </li> <li>Documentation &amp; Gouvernance : Chaque colonne est document\u00e9e dans dbt, facilitant l'onboarding des analystes et la compr\u00e9hension des KPIs.</li> </ul>"},{"location":"projets/#resultats-impact","title":"\ud83d\udcc8 R\u00e9sultats &amp; Impact","text":"<ul> <li>Fiabilit\u00e9 : D\u00e9tection proactive des erreurs d'API avant qu'elles n'atteignent les rapports m\u00e9tier.</li> <li>Agilit\u00e9 : Passage d'une gestion manuelle par exports CSV \u00e0 une plateforme 100% automatis\u00e9e.</li> <li>Scalabilit\u00e9 : Architecture pr\u00eate \u00e0 accueillir de nouvelles sources ou des mod\u00e8les de pr\u00e9diction (Machine Learning).</li> </ul>"},{"location":"projets/#liens-du-projet","title":"\ud83d\udd17 Liens du projet","text":"<ul> <li>Code Source sur GitHub : Exploration de l'architecture, des DAGs Airflow et des mod\u00e8les dbt.</li> <li>Documentation Technique sur Github : D\u00e9tails des choix d'ing\u00e9nierie et de la mod\u00e9lisation des donn\u00e9es.</li> </ul>"}]}